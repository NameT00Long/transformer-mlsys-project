import torch
import pickle
import sys
import os

# --- æ–°å¢/ä¿®æ”¹çš„éƒ¨åˆ†ï¼šè‡ªåŠ¨è·å–é¡¹ç›®æ ¹ç›®å½•å¹¶æ·»åŠ åˆ°æœç´¢è·¯å¾„ ---
# è·å–å½“å‰æ–‡ä»¶ï¼ˆæ¯”å¦‚ inference.pyï¼‰çš„ç»å¯¹è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
# å‡è®¾ model æ–‡ä»¶å¤¹å’Œå½“å‰è„šæœ¬åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹ï¼Œé‚£ä¹ˆé¡¹ç›®æ ¹ç›®å½•å°±æ˜¯ current_dir
project_root = current_dir 

# å°† project_root åŠ å…¥ç³»ç»Ÿè·¯å¾„ï¼Œè§£å†³ ModuleNotFoundError
if project_root not in sys.path:
    sys.path.append(project_root)
# -------------------------------------------------------

# ç°åœ¨è¿™è¡Œå°±ä¸ä¼šæŠ¥é”™äº†
from model.full_transformer import Transformer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åŠ è½½è¯è¡¨ï¼ˆå‡è®¾æ˜¯ dict: token -> idxï¼Œå¹¶ä¿å­˜äº† idx->token åå‘è¡¨æˆ–å¯åå‘ï¼‰
try:
    with open(os.path.join(project_root, 'src_vocab.pkl'), 'rb') as f:
        src_vocab = pickle.load(f)
    with open(os.path.join(project_root, 'tgt_vocab.pkl'), 'rb') as f:
        tgt_vocab = pickle.load(f)
except FileNotFoundError:
    print("âŒ è¯æ±‡è¡¨æ–‡ä»¶æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆè¯æ±‡è¡¨ã€‚")
    exit(1)

# ç¡®ä¿è¯æ±‡è¡¨æ˜¯æ­£ç¡®çš„æ ¼å¼
if hasattr(src_vocab, 'token_to_idx'):  # å¦‚æœæ˜¯è‡ªå®šä¹‰Vocabç±»
    token2idx_src = src_vocab.token_to_idx
    idx2token_src = src_vocab.idx_to_token
else:  # å¦‚æœæ˜¯å­—å…¸
    token2idx_src = src_vocab
    idx2token_src = {i:t for t,i in token2idx_src.items()}

if hasattr(tgt_vocab, 'token_to_idx'):  # å¦‚æœæ˜¯è‡ªå®šä¹‰Vocabç±»
    token2idx_tgt = tgt_vocab.token_to_idx
    idx2token_tgt = tgt_vocab.idx_to_token
else:  # å¦‚æœæ˜¯å­—å…¸
    token2idx_tgt = tgt_vocab
    idx2token_tgt = {i:t for t,i in token2idx_tgt.items()}

# è°ƒæ•´ä»¥ä¸‹ç‰¹æ®Š token id ä¸ºä½ è¯è¡¨ä¸­çš„çœŸå® id
PAD_IDX = 0
SOS_IDX = token2idx_tgt.get('<sos>', 1)
EOS_IDX = token2idx_tgt.get('<eos>', 2)
UNK_IDX = token2idx_src.get('<unk>', 3)

# æ¢å¤æ¨¡å‹ç»“æ„å¹¶åŠ è½½æƒé‡
# ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„å‚æ•°
d_model = 512  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
num_heads = 8
num_layers = 3
d_ff = d_model * 4  # é€šå¸¸ä¸º d_model çš„ 4 å€
max_len = 512
dropout = 0.1

model = Transformer(
    src_vocab_size=len(idx2token_src),
    tgt_vocab_size=len(idx2token_tgt),
    d_model=d_model,
    num_heads=num_heads,
    d_ff=d_ff,
    num_layers=num_layers,
    max_len=max_len,
    dropout=dropout
).to(device)

try:
    model_path = os.path.join(project_root, 'translation_model.pth')
    ckpt = torch.load(model_path, map_location=device)
    # å¦‚æœ ckpt æ˜¯ state_dict æˆ– åŒ…å« 'model_state_dict'
    if isinstance(ckpt, dict) and 'model_state_dict' in ckpt:
        model.load_state_dict(ckpt['model_state_dict'])
    elif isinstance(ckpt, dict) and any(k.startswith('encoder') or k.startswith('decoder') for k in ckpt.keys()):
        model.load_state_dict(ckpt)
    else:
        # å…¼å®¹ç›´æ¥ä¿å­˜æ•´ä¸ªæ¨¡å‹çš„æƒ…å†µ
        model.load_state_dict(ckpt)
except FileNotFoundError:
    print("âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹ã€‚")
    exit(1)

model.eval()

# ç®€å• tokenizer: ç©ºæ ¼åˆ†è¯ï¼ˆè¯·æ›¿æ¢ä¸º data/translation_data.py ä¸­ä¸€è‡´çš„ tokenizerï¼‰
def encode_src(text, max_len=50):
    tokens = text.strip().split()
    ids = [token2idx_src.get(t, UNK_IDX) for t in tokens][:max_len]
    return torch.tensor([ids], dtype=torch.long, device=device)  # shape (1, L)

# è´ªå¿ƒè§£ç ï¼ˆé€æ­¥ç”Ÿæˆï¼‰
@torch.no_grad()
def translate(text, max_len=50):
    src_ids = encode_src(text, max_len=max_len)  # (1, L)
    # åˆå§‹ target è¾“å…¥ä¸º SOS
    tgt_ids = torch.tensor([[SOS_IDX]], dtype=torch.long, device=device)
    for _ in range(max_len):
        out = model(src_ids, tgt_ids)  # æœŸæœ›æ¨¡å‹è¿”å› logits (B, T, V)
        next_logits = out[:, -1, :]    # (B, V)
        next_tok = next_logits.argmax(-1).unsqueeze(1)  # (B,1)
        tgt_ids = torch.cat([tgt_ids, next_tok], dim=1)
        if next_tok.item() == EOS_IDX:
            break
    # è½¬å›æ–‡æœ¬ï¼ˆä¸åŒ…å« SOSï¼‰
    ids = tgt_ids[0].cpu().tolist()[1:]
    tokens = []
    for i in ids:
        if i == EOS_IDX: break
        tokens.append(idx2token_tgt[i] if i < len(idx2token_tgt) else '<unk>')
    return ' '.join(tokens)

# ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸš€ ç¿»è¯‘æ¨¡å‹æ¨ç†å¼€å§‹...")
    print("è¯·è¾“å…¥è¦ç¿»è¯‘çš„è‹±æ–‡å¥å­ï¼ˆè¾“å…¥'quit'é€€å‡ºï¼‰:")
    
    while True:
        text = input("\nè¾“å…¥: ")
        if text.lower() == 'quit':
            break
        if text.strip():
            translation = translate(text)
            print(f"è¾“å‡º: {translation}")