"""
ç¿»è¯‘æ¨ç†æ¨¡å— - å·²ä¿®å¤
"""
import os
import sys
import pickle
import torch
import re  # å¯¼å…¥æ­£åˆ™ç”¨äºç®€å•åˆ†è¯
from model.full_transformer import Transformer

# è·å–é¡¹ç›®æ ¹ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = current_dir

if project_root not in sys.path:
    sys.path.append(project_root)

# æ¨¡å‹å‚æ•°
d_model = 256        # æ ¹æ®æŠ¥é”™ä¿®æ­£ï¼š256
num_layers = 3       # æ ¹æ®æŠ¥é”™ä¿®æ­£ï¼š3 (å› ä¸ºç¼ºå¤±äº† layer 3,4,5)
num_heads = 8        # ä¿æŒé»˜è®¤ï¼Œé€šå¸¸ d_model(256) èƒ½è¢« 8 æ•´é™¤ (256/8=32)ï¼Œåº”è¯¥æ²¡é—®é¢˜
                     # å¦‚æœå†æ¬¡æŠ¥é”™ï¼Œå°è¯•æ”¹ä¸º 4

d_ff = d_model * 4   # è¿™ä¼šè‡ªåŠ¨å˜æˆ 1024ï¼Œä¸æŠ¥é”™ä¿¡æ¯å»åˆ
max_len = 50        # è¿™ä¸ªé€šå¸¸ä¸å½±å“æƒé‡åŠ è½½ï¼Œä¿æŒé»˜è®¤å³å¯
dropout = 0.1

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_vocab(vocab_path):
    """åŠ è½½è¯è¡¨æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    print(f"ğŸ” å°è¯•åŠ è½½è¯è¡¨æ–‡ä»¶: {vocab_path}")
    
    if not os.path.exists(vocab_path):
        print(f"âŒ é”™è¯¯: è¯è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}")
        return None, None
    
    try:
        # ä¸ºäº†è®©pickleèƒ½æ‰¾åˆ°Vocabç±»ï¼Œæœ‰æ—¶å€™éœ€è¦æŠŠå½“å‰ç›®å½•åŠ åˆ°pathï¼ˆè™½ç„¶ä¸Šé¢å·²ç»åŠ äº†ï¼‰
        # å¦‚æœpickleæŠ¥é”™ "Can't get attribute 'Vocab'..."ï¼Œéœ€è¦ç¡®ä¿åŒ…å«å®šä¹‰Vocabçš„æ–‡ä»¶åœ¨è·¯å¾„ä¸­
        with open(vocab_path, 'rb') as f:
            vocab = pickle.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½è¯è¡¨æ–‡ä»¶ï¼Œç±»å‹: {type(vocab)}")
        
        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡å‡†æ ¼å¼ {'token2idx': ...}
        if isinstance(vocab, dict):
            if 'token2idx' in vocab and 'idx2token' in vocab:
                return vocab['token2idx'], vocab['idx2token']
            return vocab, {v: k for k, v in vocab.items()}
        
        # 2. æ£€æŸ¥ token_to_idx (ä½ çš„Vocabç±»ä½¿ç”¨çš„æ ¼å¼)
        elif hasattr(vocab, 'token_to_idx') and hasattr(vocab, 'idx_to_token'):
            print(f"âœ… è¯†åˆ«åˆ° token_to_idx å±æ€§æ ¼å¼")
            return vocab.token_to_idx, vocab.idx_to_token
            
        # 3. æ£€æŸ¥ token2idx (å¦ä¸€ç§å¸¸è§æ ¼å¼)
        elif hasattr(vocab, 'token2idx') and hasattr(vocab, 'idx2token'):
            print(f"âœ… è¯†åˆ«åˆ° token2idx å±æ€§æ ¼å¼")
            return vocab.token2idx, vocab.idx2token
            
        # 4. åˆ—è¡¨æ ¼å¼
        elif isinstance(vocab, list):
            token2idx = {token: idx for idx, token in enumerate(vocab)}
            idx2token = {idx: token for idx, token in enumerate(vocab)}
            return token2idx, idx2token
            
        else:
            # å°è¯•å¼ºè½¬å­—å…¸
            try:
                token2idx = dict(vocab)
                idx2token = {v: k for k, v in token2idx.items()}
                return token2idx, idx2token
            except:
                print(f"âŒ é”™è¯¯: æ— æ³•è¯†åˆ«è¯è¡¨å¯¹è±¡å†…éƒ¨ç»“æ„: {dir(vocab)}")
                return None, None
                
    except Exception as e:
        print(f"âŒ åŠ è½½è¯è¡¨æ—¶å‡ºé”™: {e}")
        return None, None

def load_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    token2idx_src, idx2token_src = load_vocab(os.path.join(project_root, 'src_vocab.pkl'))
    token2idx_tgt, idx2token_tgt = load_vocab(os.path.join(project_root, 'tgt_vocab.pkl'))
    
    if token2idx_src is None or token2idx_tgt is None:
        return None, None, None, None
    
    print(f"ğŸ“Š æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(token2idx_src)}")
    print(f"ğŸ“Š ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(token2idx_tgt)}")
    
    model = Transformer(
        src_vocab_size=len(token2idx_src),
        tgt_vocab_size=len(token2idx_tgt),
        d_model=d_model,
        num_heads=num_heads,
        d_ff=d_ff,
        num_layers=num_layers,
        max_len=max_len,
        dropout=dropout
    ).to(device)
    
    model_path = os.path.join(project_root, 'translation_model.pth')
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")
        return None, None, None, None
        
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"âœ… æ¨¡å‹æƒé‡å·²åŠ è½½")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None
    
    model.eval()
    return model, token2idx_src, token2idx_tgt, idx2token_tgt

def translate(text, model, token2idx_src, token2idx_tgt, idx2token_tgt):
    """
    ä¿®æ­£ç‰ˆç¿»è¯‘å‡½æ•°ï¼šæ·»åŠ äº†ç¼ºå¤±çš„ Source SOS æ ‡è®°
    """
    # 1. è·å–ç‰¹æ®Štokenç´¢å¼•
    # å‡è®¾æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„ç‰¹æ®Štokenå­—ç¬¦ä¸²æ˜¯ä¸€æ ·çš„
    # å¦‚æœä½ çš„ src_vocab é‡Œæ²¡æœ‰ <sos>ï¼Œé€šå¸¸å¯ä»¥ç”¨ <unk> ä»£æ›¿ï¼Œä½†æ ¹æ®ä½ çš„ä»£ç æ¥çœ‹æ˜¯æœ‰çš„
    SRC_SOS_IDX = token2idx_src.get('<sos>', token2idx_src.get('<unk>', 1))
    SRC_UNK_IDX = token2idx_src.get('<unk>', 3)
    
    TGT_SOS_IDX = token2idx_tgt.get('<sos>', 1)
    TGT_EOS_IDX = token2idx_tgt.get('<eos>', 2)
    
    # print(f"\n[DEBUG] Src SOS={SRC_SOS_IDX}, Tgt SOS={TGT_SOS_IDX}, EOS={TGT_EOS_IDX}")
    
    # 2. åˆ†è¯ä¸æ˜ å°„
    import re
    # ç®€å•åˆ†è¯ï¼šæŠŠæ ‡ç‚¹éš”å¼€
    text = re.sub(r"([?.!,])", r" \1 ", text)
    tokens = text.lower().split()
    
    # 3. æ„å»ºæºè¯­è¨€åºåˆ— [SOS, ... tokens ..., EOS]
    src_indices = [SRC_SOS_IDX]  # <--- ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ  SOS åˆ°å¼€å¤´
    
    for token in tokens:
        idx = token2idx_src.get(token, SRC_UNK_IDX)
        src_indices.append(idx)
    
    # æ·»åŠ ç»“æŸæ ‡è®° (æ ¹æ®ä½ çš„Datasetä»£ç ï¼Œæºè¯­è¨€æœ«å°¾ä¹Ÿæœ‰EOS)
    src_indices.append(token2idx_src.get('<eos>', 2)) 
    
    print(f"[DEBUG] æºè¯­è¨€Tensorç´¢å¼• (ä¿®æ­£å): {src_indices}")
    
    # 4. å‡†å¤‡å¼ é‡
    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)
    
    # ç›®æ ‡åºåˆ—åˆå§‹åŒ– [SOS]
    tgt_indices = [TGT_SOS_IDX]
    
    with torch.no_grad():
        for i in range(max_len):
            tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)
            
            # å‰å‘ä¼ æ’­
            output = model(src_tensor, tgt_tensor)
            
            # è·å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹
            last_token_logits = output[0, -1, :]
            
            # è´ªå©ªæœç´¢: å–æ¦‚ç‡æœ€å¤§çš„è¯
            next_token = last_token_logits.argmax().item()
            
            # --- è°ƒè¯•æ‰“å° (å¯é€‰) ---
            # probs = torch.softmax(last_token_logits, dim=0)
            # top3_prob, top3_idx = torch.topk(probs, 3)
            # top3_words = [idx2token_tgt[idx.item()] if 0 <= idx.item() < len(idx2token_tgt) else 'ERR' for idx in top3_idx]
            # print(f"[DEBUG] Step {i}: é¢„æµ‹={top3_words[0]}")
            # ---------------------
            
            if next_token == TGT_EOS_IDX:
                break
            
            tgt_indices.append(next_token)
    
    # 5. ç»“æœè½¬æ¢ (è·³è¿‡å¼€å¤´çš„ SOS)
    translation_tokens = []
    for idx in tgt_indices[1:]:
        if 0 <= idx < len(idx2token_tgt):
            translation_tokens.append(idx2token_tgt[idx])
        else:
            translation_tokens.append('<unk>')
        
    return ' '.join(translation_tokens)    
    """
    å·²ä¿®å¤åˆ—è¡¨è®¿é—®é”™è¯¯çš„ç¿»è¯‘å‡½æ•°
    """
    # 1. è·å–ç‰¹æ®Štokenç´¢å¼•
    # æ³¨æ„ï¼štoken2idx æ˜¯å­—å…¸ï¼Œæ‰€ä»¥è¿™é‡Œç”¨ .get() æ˜¯æ­£ç¡®çš„
    PAD_IDX = token2idx_src.get('<pad>', 0)
    SOS_IDX = token2idx_tgt.get('<sos>', 1)
    EOS_IDX = token2idx_tgt.get('<eos>', 2)
    UNK_IDX = token2idx_src.get('<unk>', 3)
    
    print(f"\n[DEBUG] ç‰¹æ®ŠTokenç´¢å¼•: SOS={SOS_IDX}, EOS={EOS_IDX}, UNK={UNK_IDX}")
    
    # 2. åˆ†è¯ä¸æ˜ å°„
    import re
    text = re.sub(r"([?.!,])", r" \1 ", text)
    tokens = text.lower().split()
    
    print(f"[DEBUG] åˆ†è¯ç»“æœ: {tokens}")
    
    src_indices = []
    for token in tokens:
        idx = token2idx_src.get(token, UNK_IDX)
        src_indices.append(idx)
        if idx == UNK_IDX:
            print(f"[DEBUG] âš ï¸ è­¦å‘Š: å•è¯ '{token}' æœªåœ¨è¯è¡¨ä¸­æ‰¾åˆ°ï¼Œè½¬æ¢ä¸º UNK")
    
    src_indices.append(EOS_IDX)
    print(f"[DEBUG] æºè¯­è¨€Tensorç´¢å¼•: {src_indices}")
    
    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)
    tgt_indices = [SOS_IDX]
    
    print(f"[DEBUG] å¼€å§‹ç”Ÿæˆ (æœ€å¤§é•¿åº¦ {max_len})...")
    
    with torch.no_grad():
        for i in range(max_len):
            tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)
            
            # å‰å‘ä¼ æ’­
            output = model(src_tensor, tgt_tensor)
            
            # è·å–é¢„æµ‹ç»“æœ
            last_token_logits = output[0, -1, :]
            
            # --- ä¿®å¤ 1ï¼šè°ƒè¯•æ‰“å° ---
            probs = torch.softmax(last_token_logits, dim=0)
            top3_prob, top3_idx = torch.topk(probs, 3)
            current_top1 = top3_idx[0].item()
            
            # å®‰å…¨åœ°ä»åˆ—è¡¨ä¸­è·å–å•è¯ç”¨äºæ˜¾ç¤º
            top3_words = []
            for idx_tensor in top3_idx:
                idx = idx_tensor.item()
                if 0 <= idx < len(idx2token_tgt):
                    top3_words.append(idx2token_tgt[idx])  # ä½¿ç”¨åˆ—è¡¨ç´¢å¼•è®¿é—®
                else:
                    top3_words.append('<out_of_bounds>')

            print(f"[DEBUG] Step {i}: å½“å‰åºåˆ—={tgt_indices}, é¢„æµ‹Top3={top3_words} (IDs: {top3_idx.tolist()})")
            
            next_token = current_top1
            
            if next_token == EOS_IDX:
                print(f"[DEBUG] ğŸ›‘ æ¨¡å‹ç”Ÿæˆäº† EOSï¼Œåœæ­¢ç”Ÿæˆã€‚")
                break
            
            tgt_indices.append(next_token)
    
    # --- ä¿®å¤ 2ï¼šç»“æœè½¬æ¢ ---
    translation_tokens = []
    for idx in tgt_indices[1:]: # è·³è¿‡å¼€å¤´çš„SOS
        # ä½¿ç”¨åˆ—è¡¨ç´¢å¼•è®¿é—®
        if 0 <= idx < len(idx2token_tgt):
            token = idx2token_tgt[idx]
        else:
            token = '<unk>'
        translation_tokens.append(token)
        
    translation = ' '.join(translation_tokens)
    return translation    
    """
    å¸¦è°ƒè¯•ä¿¡æ¯çš„ç¿»è¯‘å‡½æ•°
    """
    # 1. è·å–ç‰¹æ®Štokenç´¢å¼•ï¼Œå¹¶æ‰“å°å‡ºæ¥æ ¸å¯¹
    PAD_IDX = token2idx_src.get('<pad>', 0)
    SOS_IDX = token2idx_tgt.get('<sos>', 1)
    EOS_IDX = token2idx_tgt.get('<eos>', 2)
    UNK_IDX = token2idx_src.get('<unk>', 3)
    
    print(f"\n[DEBUG] ç‰¹æ®ŠTokenç´¢å¼•: SOS={SOS_IDX}, EOS={EOS_IDX}, UNK={UNK_IDX}")
    
    # 2. åˆ†è¯ä¸æ˜ å°„
    import re
    text = re.sub(r"([?.!,])", r" \1 ", text)
    tokens = text.lower().split()
    
    # æ‰“å°åŸå§‹åˆ†è¯
    print(f"[DEBUG] åˆ†è¯ç»“æœ: {tokens}")
    
    src_indices = []
    for token in tokens:
        idx = token2idx_src.get(token, UNK_IDX)
        src_indices.append(idx)
        # å¦‚æœæ˜¯UNKï¼Œæ‰“å°è­¦å‘Š
        if idx == UNK_IDX:
            print(f"[DEBUG] âš ï¸ è­¦å‘Š: å•è¯ '{token}' æœªåœ¨è¯è¡¨ä¸­æ‰¾åˆ°ï¼Œè½¬æ¢ä¸º UNK (id={UNK_IDX})")
    
    src_indices.append(EOS_IDX)
    print(f"[DEBUG] æºè¯­è¨€Tensorç´¢å¼•: {src_indices}")
    
    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)
    tgt_indices = [SOS_IDX]
    
    print(f"[DEBUG] å¼€å§‹ç”Ÿæˆ (æœ€å¤§é•¿åº¦ {max_len})...")
    
    with torch.no_grad():
        for i in range(max_len):
            tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)
            
            # å‰å‘ä¼ æ’­
            output = model(src_tensor, tgt_tensor)
            
            # è·å–é¢„æµ‹ç»“æœ
            # output: [1, seq_len, vocab_size]
            last_token_logits = output[0, -1, :]
            
            # æ‰“å°Top 3é¢„æµ‹ï¼ˆçœ‹çœ‹æ¨¡å‹æœ€æƒ³è¾“å‡ºä»€ä¹ˆï¼‰
            probs = torch.softmax(last_token_logits, dim=0)
            top3_prob, top3_idx = torch.topk(probs, 3)
            
            current_top1 = top3_idx[0].item()
            
            print(f"[DEBUG] Step {i}: è¾“å…¥={tgt_indices}, é¢„æµ‹Top3={[idx2token_tgt.get(x.item(), str(x.item())) for x in top3_idx]} (IDs: {top3_idx.tolist()})")
            
            next_token = current_top1
            
            # è¿™é‡Œçš„åˆ¤æ–­éå¸¸å…³é”®
            if next_token == EOS_IDX:
                print(f"[DEBUG] ğŸ›‘ æ¨¡å‹ç”Ÿæˆäº† EOS (id={EOS_IDX})ï¼Œåœæ­¢ç”Ÿæˆã€‚")
                break
            
            tgt_indices.append(next_token)
    
    # è½¬æ¢ç»“æœ
    translation_tokens = []
    for idx in tgt_indices[1:]: # è·³è¿‡å¼€å¤´çš„SOS
        token = idx2token_tgt.get(idx, '<unk>') # å®‰å…¨è·å–
        translation_tokens.append(token)
        
    translation = ' '.join(translation_tokens)
    return translation    # ç‰¹æ®Štokenå¤„ç†
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨äº† .get()ï¼Œå¦‚æœè¯è¡¨ä¸­keyä¸åŒï¼Œéœ€è¦è°ƒæ•´ï¼Œä½†é€šå¸¸ key éƒ½æ˜¯å­—ç¬¦ä¸²
    PAD_IDX = token2idx_src.get('<pad>', 0)
    SOS_IDX = token2idx_tgt.get('<sos>', 1)
    EOS_IDX = token2idx_tgt.get('<eos>', 2)
    UNK_IDX = token2idx_src.get('<unk>', 3)
    
    # ç®€å•åˆ†è¯ä¼˜åŒ–
    text = re.sub(r"([?.!,])", r" \1 ", text)
    tokens = text.lower().split()
    
    src_indices = [token2idx_src.get(token, UNK_IDX) for token in tokens]
    src_indices.append(EOS_IDX)
    
    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)
    tgt_indices = [SOS_IDX]
    
    with torch.no_grad():
        for i in range(max_len):
            tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)
            output = model(src_tensor, tgt_tensor)
            
            # --- ä¿®å¤åçš„é¢„æµ‹é€»è¾‘ ---
            # å– batch=0, æœ€åä¸€ä¸ªæ—¶é—´æ­¥, argmax
            next_token = output[0, -1, :].argmax().item()
            
            tgt_indices.append(next_token)
            if next_token == EOS_IDX:
                break
    
    translation_tokens = [idx2token_tgt[idx] for idx in tgt_indices[1:]]
    # å»æ‰EOSå¦‚æœå­˜åœ¨
    if translation_tokens and translation_tokens[-1] == '<eos>':
        translation_tokens = translation_tokens[:-1]
        
    return ' '.join(translation_tokens)

if __name__ == "__main__":
    print("ğŸš€ ç¿»è¯‘æ¨¡å‹æ¨ç†å¼€å§‹...")
    model, token2idx_src, token2idx_tgt, idx2token_tgt = load_model()
    
    if model:
        print("\nâœ… ç³»ç»Ÿå°±ç»ª. è¯·è¾“å…¥è‹±æ–‡å¥å­ (è¾“å…¥ 'quit' é€€å‡º):")
        while True:
            text = input("\nè¾“å…¥: ")
            if text.lower() == 'quit': break
            if text.strip():
                try:
                    res = translate(text, model, token2idx_src, token2idx_tgt, idx2token_tgt)
                    print(f"è¾“å‡º: {res}")
                except Exception as e:
                    print(f"âŒ æ¨ç†å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()