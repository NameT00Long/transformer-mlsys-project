# test_layers.py
import torch
from model.layers import PositionalEncoding, MultiHeadAttention

def test_positional_encoding():
    print("ğŸ” æµ‹è¯• PositionalEncoding...")
    d_model = 256
    seq_len = 10
    batch_size = 2
    
    pe = PositionalEncoding(d_model)
    x = torch.randn(batch_size, seq_len, d_model)
    out = pe(x)
    
    assert out.shape == x.shape, f"å½¢çŠ¶é”™è¯¯ï¼æœŸæœ› {x.shape}, å¾—åˆ° {out.shape}"
    print("âœ… PositionalEncoding æµ‹è¯•é€šè¿‡ï¼\n")

def test_multi_head_attention():
    print("ğŸ” æµ‹è¯• MultiHeadAttention...")
    d_model = 256
    num_heads = 4
    seq_len = 10
    batch_size = 2
    
    mha = MultiHeadAttention(d_model, num_heads)
    x = torch.randn(batch_size, seq_len, d_model)
    
    # åˆ†ç±»ä»»åŠ¡ï¼šq=k=v=x
    out = mha(x, x, x)
    
    assert out.shape == x.shape, f"å½¢çŠ¶é”™è¯¯ï¼æœŸæœ› {x.shape}, å¾—åˆ° {out.shape}"
    print("âœ… MultiHeadAttention æµ‹è¯•é€šè¿‡ï¼\n")

if __name__ == "__main__":
    test_positional_encoding()
    test_multi_head_attention()
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼layers.py å·²å‡†å¤‡å¥½ï¼")