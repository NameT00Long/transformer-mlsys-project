"""
ç¿»è¯‘æ•°æ®å¤„ç†æ¨¡å—
"""
import os

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import re
import torch
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
from collections import Counter
import torch.nn.functional as F

def simple_tokenizer(text):
    """
    ç»Ÿä¸€çš„ç®€å•çš„åˆ†è¯å™¨ï¼š
    1. è½¬å°å†™
    2. å°†æ ‡ç‚¹ç¬¦å·ä¸å•è¯åˆ†å¼€
    3. æŒ‰ç©ºæ ¼åˆ‡åˆ†
    """
    if not isinstance(text, str):
        return []
    
    text = text.lower().strip()
    
    # åœ¨æ ‡ç‚¹ç¬¦å·å‰ååŠ ç©ºæ ¼ (åŒ…æ‹¬ ?.!,)
    # è¿™æ · "hello." å°±ä¼šå˜æˆ "hello ."
    text = re.sub(r"([?.!,Â¿])", r" \1 ", text)
    
    # å°†å¤šä½™çš„ç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
    text = re.sub(r'[" "]+', " ", text)
    
    return text.split()

class TranslationDataset(Dataset):
    def __init__(self, dataset, src_lang, tgt_lang, src_vocab, tgt_vocab, max_len=100):
        self.dataset = dataset
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.max_len = max_len

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        
        # æå–æ–‡æœ¬
        src_text = self._extract_text(item, self.src_lang)
        tgt_text = self._extract_text(item, self.tgt_lang)

        # --- ä¿®æ”¹å¤„ï¼šä½¿ç”¨ç»Ÿä¸€çš„åˆ†è¯å™¨ ---
        src_word_list = simple_tokenizer(src_text)
        tgt_word_list = simple_tokenizer(tgt_text)

        # ç¼–ç  (æˆªæ–­é•¿åº¦ï¼Œç•™å‡ºSOSå’ŒEOSçš„ä½ç½®)
        src_tokens = [self.src_vocab.get('<sos>', 1)] + \
                     [self.src_vocab.get(token, 3) for token in src_word_list[:self.max_len-2]] + \
                     [self.src_vocab.get('<eos>', 2)]
        
        tgt_tokens = [self.tgt_vocab.get('<sos>', 1)] + \
                     [self.tgt_vocab.get(token, 3) for token in tgt_word_list[:self.max_len-2]] + \
                     [self.tgt_vocab.get('<eos>', 2)]

        return torch.tensor(src_tokens, dtype=torch.long), torch.tensor(tgt_tokens, dtype=torch.long)
    
    def _extract_text(self, item, lang):
        # ... (ä¿æŒåŸæ¥çš„æå–é€»è¾‘ä¸å˜) ...
        if isinstance(item, dict):
            if lang in item: return item[lang]
            elif 'translation' in item and lang in item['translation']: return item['translation'][lang]
        return ""



class Vocab:
    """è¯æ±‡è¡¨ç±»"""
    def __init__(self, tokens=None, max_tokens=None):
        self.token_to_idx = {}
        self.idx_to_token = []
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        self.pad_token = '<pad>'  # ç´¢å¼•0
        self.sos_token = '<sos>'  # ç´¢å¼•1
        self.eos_token = '<eos>'  # ç´¢å¼•2
        self.unk_token = '<unk>'  # ç´¢å¼•3

        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        special_tokens = [self.pad_token, self.sos_token, self.eos_token, self.unk_token]
        for token in special_tokens:
            self.token_to_idx[token] = len(self.idx_to_token)
            self.idx_to_token.append(token)

        if tokens:
            self.build_vocab(tokens, max_tokens)

    def build_vocab(self, tokens, max_tokens=None):
        """æ„å»ºè¯æ±‡è¡¨"""
        counter = Counter(tokens)
        sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        
        for token, _ in sorted_tokens:
            if token not in self.token_to_idx:
                self.token_to_idx[token] = len(self.idx_to_token)
                self.idx_to_token.append(token)
                if max_tokens and len(self.idx_to_token) >= max_tokens:
                    break

    def __len__(self):
        return len(self.idx_to_token)

    def get(self, token, default=None):
        """è·å–tokençš„ç´¢å¼•"""
        if default is None:
            return self.token_to_idx.get(token, self.token_to_idx[self.unk_token])
        else:
            return self.token_to_idx.get(token, default)

    def to_tokens(self, indices):
        """å°†ç´¢å¼•è½¬æ¢ä¸ºtoken"""
        return [self.idx_to_token[idx] for idx in indices]


def build_vocab_from_dataset(dataset, src_lang, tgt_lang, max_tokens=30000):
    """ä»æ•°æ®é›†æ„å»ºè¯æ±‡è¡¨"""
    print("ğŸ”„ æ­£åœ¨æ„å»ºè¯æ±‡è¡¨ (ä½¿ç”¨ simple_tokenizer)...")
    src_counter = Counter()
    tgt_counter = Counter()
    
    # éå†æ•°æ®é›† (IWSLTä¸ç®—å¤ªå¤§ï¼Œå»ºè®®éå†å…¨éƒ¨æˆ–è‡³å°‘5ä¸‡æ¡)
    # å¦‚æœä¸ºäº†é€Ÿåº¦ï¼Œå¯ä»¥åªéå†å‰ 30000 æ¡
    limit = min(30000, len(dataset)) 
    
    for i in range(limit):
        item = dataset[i]
        src_text = extract_text_from_item(item, src_lang)
        tgt_text = extract_text_from_item(item, tgt_lang)
        
        # --- ä¿®æ”¹å¤„ï¼šä½¿ç”¨ç»Ÿä¸€çš„åˆ†è¯å™¨è¿›è¡Œç»Ÿè®¡ ---
        src_counter.update(simple_tokenizer(src_text))
        tgt_counter.update(simple_tokenizer(tgt_text))
    
    # æå–æœ€å¸¸è§çš„å•è¯åˆ—è¡¨
    src_tokens_list = [token for token, count in src_counter.most_common(max_tokens)]
    tgt_tokens_list = [token for token, count in tgt_counter.most_common(max_tokens)]

    # æ„å»º
    src_vocab = Vocab(src_tokens_list, max_tokens=max_tokens)
    tgt_vocab = Vocab(tgt_tokens_list, max_tokens=max_tokens)
    
    return src_vocab, tgt_vocab


def extract_text_from_item(item, lang):
    """ä»æ•°æ®é¡¹ä¸­æå–æ–‡æœ¬ï¼Œå¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼"""
    # å¦‚æœæ˜¯ç®€å•çš„é”®å€¼å¯¹æ ¼å¼
    if isinstance(item, dict):
        if lang in item:
            return item[lang]
        # å¦‚æœæ˜¯translationæ ¼å¼
        elif 'translation' in item and lang in item['translation']:
            return item['translation'][lang]
        # å¦‚æœæ˜¯sentenceæ ¼å¼
        elif 'sentence' in item and lang == 'sentence':
            return item['sentence']
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”è¯­è¨€çš„æ–‡æœ¬ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    return ""


def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°ï¼Œç”¨äºå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—"""
    src_batch, tgt_batch = zip(*batch)
    
    # æ‰¾åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦
    src_max_len = max([len(seq) for seq in src_batch])
    tgt_max_len = max([len(seq) for seq in tgt_batch])
    
    # å¡«å……åˆ°ç›¸åŒé•¿åº¦
    src_padded = [F.pad(seq, (0, src_max_len - len(seq)), value=0) for seq in src_batch]
    tgt_padded = [F.pad(seq, (0, tgt_max_len - len(seq)), value=0) for seq in tgt_batch]
    
    return torch.stack(src_padded), torch.stack(tgt_padded)


def get_dataloaders(data_dir, batch_size=64, max_len=100, src_lang='en', tgt_lang='de', max_tokens=30000):
    """
    ä½¿ç”¨ Opus Books æ•°æ®é›† (å°è¯´/çŸ­å¥)
    ä¼˜ç‚¹ï¼šä¸‹è½½æå…¶ç¨³å®šï¼Œæ— éœ€è„šæœ¬ï¼Œé€‚åˆè¯¾ç¨‹ä½œä¸š
    """
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½ Opus Books (en-de) æ•°æ®é›†...")
    from datasets import load_dataset
    
    try:
        # åŠ è½½ Opus Books
        # è¿™æ˜¯ä¸€ä¸ªçº¯æ•°æ®æ–‡ä»¶ï¼Œé•œåƒç«™ä¸‹è½½éå¸¸å¿«ï¼Œä¸”ä¸éœ€è¦ trust_remote_code
        dataset = load_dataset("opus_books", "de-en")
        
        print(f"ğŸ’¡ æ•°æ®é›†åŠ è½½æˆåŠŸ (å¤§å°: {len(dataset['train'])})ï¼Œæ­£åœ¨æ„å»ºè¯è¡¨...")
        
        # Opus Books åªæœ‰ 'train' åˆ†å‰²
        # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨åˆ‡åˆ†å‡º éªŒè¯é›† å’Œ æµ‹è¯•é›†
        # 90% è®­ç»ƒ, 5% éªŒè¯, 5% æµ‹è¯•
        full_dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)
        train_set = full_dataset['train']
        
        test_val_split = full_dataset['test'].train_test_split(test_size=0.5, seed=42)
        val_set = test_val_split['train']
        test_set = test_val_split['test']
        
        # æ„å»ºè¯è¡¨ (ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„ build_vocab_from_dataset)
        # æ³¨æ„ï¼šOpus çš„æ•°æ®ç»“æ„æ˜¯ item['translation']['en']
        src_vocab, tgt_vocab = build_vocab_from_dataset(
            train_set, 'en', 'de', max_tokens=max_tokens
        )
        
        # åˆ›å»º Dataset å¯¹è±¡
        train_dataset = TranslationDataset(train_set, 'en', 'de', src_vocab, tgt_vocab, max_len)
        val_dataset = TranslationDataset(val_set, 'en', 'de', src_vocab, tgt_vocab, max_len)
        test_dataset = TranslationDataset(test_set, 'en', 'de', src_vocab, tgt_vocab, max_len)
        
        # åˆ›å»º DataLoader
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
        
        return train_loader, val_loader, test_loader, src_vocab, tgt_vocab

    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None, None